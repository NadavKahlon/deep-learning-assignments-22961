{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1stAssignmentDL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pHnH2H6iRc4e",
        "UXM2cQCHRk89",
        "ANA3m1bhVNBF",
        "3j4HaPwie8HO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Student Info & Imports"
      ],
      "metadata": {
        "id": "pHnH2H6iRc4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student info: Nadav Kahlon, ID: 213438575"
      ],
      "metadata": {
        "id": "Nnk3WCw7Riv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary packages\n",
        "import torch\n",
        "import math"
      ],
      "metadata": {
        "id": "X-6y5fwoJbt9"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n",
        "In this question I implement and demonstrate a PyTorch tensor constructor that samples its elements from a discrete probability distribution. I decided to use the **alias method** to generate the tensor's elements efficiently, as described [here](https://en.wikipedia.org/wiki/Alias_method)."
      ],
      "metadata": {
        "id": "UXM2cQCHRk89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use 2 functions to complete this task:\n",
        "* 'alias_preproc' generates the probability table U and the alias table K (see [here](https://en.wikipedia.org/wiki/Alias_method#Table_generation)).\n",
        "* 'my_sampler' samples from the distribution using these tables, and creates the required tensor.\n",
        "\n",
        "The next cell includes their definitions."
      ],
      "metadata": {
        "id": "B7U0zjtenUr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "alias_preproc : preprocessing of the alias method for sampling data from discrete\n",
        "  distribution.\n",
        "Input:\n",
        "  > 'dist' - a tensor of floats of shape (n) representing the distribution;\n",
        "    for each i, dist[i] represents the probability to sample i.\n",
        "Returns: (U, K), where:\n",
        "  > 'U' is the probability table used by the alias method, represented by a tensor\n",
        "    of floats of shape (n).\n",
        "  > 'K' is the alias table used by the alias method, represented by a tensor of\n",
        "    (long) integers of shape (n). Uninitialized entries will be -1.\n",
        "Note: the function ASSUMES that the distribution (the input) is valid.\n",
        "For more detail regarding the alias method, visit: \n",
        "    https://en.wikipedia.org/wiki/Alias_method\n",
        "'''\n",
        "def alias_preproc(dist):\n",
        "    # initialize output\n",
        "    n = dist.numel()\n",
        "    U = n * dist\n",
        "    K = torch.full_like(dist, dtype=torch.long,\n",
        "                        fill_value=-1) # -1 represents \"not yet initialized\"\n",
        "\n",
        "    # run the preprocessing algorithm, and stop when all entries are full; it is \n",
        "    # guaranteed to finish after n-1 iterations\n",
        "    for i in range(dist.numel()-1):\n",
        "\n",
        "        # end if all entries are \"exactly full\":\n",
        "        if torch.all(torch.logical_or(U == 1.0, K >= 0)):\n",
        "            break\n",
        "        \n",
        "        # pick an overfull entry i and an underfull entry j (using a greedy method)\n",
        "        inf_tensor = torch.full_like(U, fill_value=torch.inf)\n",
        "        i = torch.argmax(U).item()\n",
        "        j = torch.argmin(torch.where(K < 0, U, inf_tensor)).item() # (note how we discard\n",
        "                                                                   # initialized entries of K)\n",
        "\n",
        "        # \"transfer probabilities\" from U[i] to U[j]\n",
        "        K[j] = i\n",
        "        U[i] = U[i] - (1-U[j])\n",
        "    \n",
        "    return U, K\n",
        "\n",
        "\n",
        "'''\n",
        "my_sampler : constructs a new PyTorch tensor whose elements are drawn from a given\n",
        "  discrete distribution.\n",
        "Input:\n",
        "  > 'size' - shape of the output tensor.\n",
        "  > 'dist' - a list (or list-like) of n floats representing the distribution;\n",
        "    for each i, dist[i] represents the probability to sample i.\n",
        "  > 'requires_grad' - if autograd should record operations on the returned tensor.\n",
        "    Default: False.\n",
        "Returns: the required tensor of shape 'size'.\n",
        "Throws as assertion error if the input distribution is not valid:\n",
        "  * 'dist' should be 1 dimensional;\n",
        "  * The elemnts of 'dist' should be non-negative, and sum-up to 1.\n",
        "'''\n",
        "def my_sampler(size, dist, requires_grad=False):\n",
        "    # for easier manipulation, we transform the distribution list to a tensor\n",
        "    dist = torch.tensor(dist)\n",
        "\n",
        "    # check validity of the distribution\n",
        "    assert (len(dist.shape) == 1 and torch.all(dist >= 0) and\n",
        "            dist.sum() == 1.0), 'the given distribution is not valid.'\n",
        "    \n",
        "    # preprocessing\n",
        "    U, K = alias_preproc(dist)\n",
        "\n",
        "    # draw 'x', 'i', and 'y' (in terms of the alias method)\n",
        "    n = dist.numel()\n",
        "    x = torch.rand(size)\n",
        "    i = torch.floor(n*x).long()\n",
        "    y = n*x - i\n",
        "\n",
        "    # use the alias tables to draw the actual output\n",
        "    output = torch.where(y < U[i], i, K[i]).float().clone().detach().requires_grad_(requires_grad)\n",
        "    \n",
        "    return output"
      ],
      "metadata": {
        "id": "5hkgMAH7TAfP"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we demonstrate the sampling function by drawing 10,000 samples from the distribution [0.1, 0.2, 0.7]:"
      ],
      "metadata": {
        "id": "S9iDyfUwnF1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample data\n",
        "dist = [0.1, 0.2, 0.7]\n",
        "size = (10000)\n",
        "samples_tensor = my_sampler(size, dist)\n",
        "\n",
        "# plot histogram\n",
        "from matplotlib import pyplot as plt\n",
        "_ = plt.hist(samples_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "I5efXvn6nrHc",
        "outputId": "b8cb3f1c-0e3a-4473-e45e-c2caa5b72a0e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More input & output examples:"
      ],
      "metadata": {
        "id": "UvPUvXGT9RJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple sampling example\n",
        "my_sampler(10, [0.5,0.5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvH8ZlyH7zkM",
        "outputId": "fe927d98-4a7a-436a-c483-40ae889b7260"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 0., 1., 0., 0., 1., 0., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# requires_grad=True example\n",
        "A = my_sampler((2,8), [0.1,0.2,0.7], requires_grad=True)\n",
        "print(A, A.grad, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzhReJqS9EGw",
        "outputId": "b8d1600b-4d01-470a-a4b5-55353966d58f"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2., 0., 2.],\n",
            "        [1., 2., 2., 2., 2., 2., 2., 2.]], requires_grad=True)\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating gradiant example\n",
        "A = my_sampler((2,8),[0.1,0.2,0.7],requires_grad=True)\n",
        "B = (A**2).sum()\n",
        "B.backward()\n",
        "print(A,A.grad,sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hBRWXXp9kxR",
        "outputId": "866009aa-8104-421e-f400-acd483772300"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2., 2., 2.],\n",
            "        [2., 1., 2., 2., 1., 1., 1., 2.]], requires_grad=True)\n",
            "tensor([[4., 4., 4., 4., 4., 4., 4., 4.],\n",
            "        [4., 2., 4., 4., 2., 2., 2., 4.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2\n",
        "In this question I implement the broadcasting functionality of PyTorch."
      ],
      "metadata": {
        "id": "ANA3m1bhVNBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part A - 'expand_as' Functionality"
      ],
      "metadata": {
        "id": "M3YrgroFVRDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I implement a mechanism to broadcast one tensor to another tensor's shape. I decided to use 2 functions to implement this:\n",
        "* 'expand_to' - to broadcast a tensor to a specific shape.\n",
        "* 'my_expand_as' - to broadcast a tensor to the shape of some other tensor.\n",
        "They ar defined in in the following cell."
      ],
      "metadata": {
        "id": "gXwwF9OoLgcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "expand_to : broadcasts a tensor to a specific shape.\n",
        "Input:\n",
        "    > 'A' - the tensor to broadcast.\n",
        "    > 'size' - the size to broadcast to.\n",
        "Output: a new tensor - the broadcasted version of 'A', or None if A is not \n",
        "    broadcastable to 'size'.\n",
        "'''\n",
        "def expand_to(A, size):\n",
        "    # allocate a new tensor to represents the output\n",
        "    A = A.clone()\n",
        "\n",
        "    # traverse 'size' and 'A.shape' backwards to make them compatible:\n",
        "    i_a = len(A.shape) - 1 # index into A's shape\n",
        "    i_s = len(size) - 1 # index into 'size'\n",
        "    while i_a >= 0 or i_s >= 0:\n",
        "\n",
        "        if i_a < 0:\n",
        "            # if reached the beginning of A's shape - add dummy singleton dimension\n",
        "            A.unsqueeze_(0)\n",
        "            i_a += 1\n",
        "        \n",
        "        # can't broadcast if 'size' is too short or if a target dimension does not\n",
        "        # match a source's non-singleton dimension\n",
        "        if i_s < 0 or (size[i_s] != A.shape[i_a] and A.shape[i_a] != 1):\n",
        "            return None\n",
        "        \n",
        "        if A.shape[i_a] == 1 and A.shape[i_a] != size[i_s]:\n",
        "            # expand A if A's singleton dimension does not match 'size' entry by\n",
        "            # concatenating A to itself on the required dimension\n",
        "            A = torch.cat([A] * size[i_s], dim=i_a)\n",
        "\n",
        "        # update indices for the next iteration\n",
        "        i_a -= 1\n",
        "        i_s -= 1\n",
        "\n",
        "    return A\n",
        "\n",
        "\n",
        "'''\n",
        "my_expand_as : broadcasts a tensor into the shape of another tensor.\n",
        "Input:\n",
        "    > 'A' - the tensor to broadcast.\n",
        "    > 'B' - the tensor whose shape is the shape to broadcast 'A' to.\n",
        "Output: a new tensor - the broadcasted version of 'A'.\n",
        "Throws an assertion error if 'A' is not  broadcastable to B's shape.\n",
        "'''\n",
        "def my_expand_as(A, B):\n",
        "    # we simply use 'expand_to' with B's shape as the target shape\n",
        "    result = expand_to(A, B.shape)\n",
        "    assert result != None, ('tensor of shape ' + str(tuple(A.shape)) +\n",
        "                            ' is not broadcastible to shape ' + str(tuple(B.shape)))\n",
        "    return result"
      ],
      "metadata": {
        "id": "kqoF9NQhWZWq"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part B - Checking Broadcastability"
      ],
      "metadata": {
        "id": "h_9sEVNh5VpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "are_broadcastible : checks if 2 tensors can be broadcasted together, and calculates\n",
        "  the broadcasted shape.\n",
        "Input:\n",
        "  > 'A' - first tensor.\n",
        "  > 'B' - second tensor\n",
        "Retruns: (is_possible, size), where:\n",
        "  > 'is_possible' is True if tensors are broadcastable together, and False otherwise.\n",
        "  > 'size' is the broadcasted shape of the tensors, or None if they are not\n",
        "      broadcastable together.\n",
        "'''\n",
        "def are_broadcastible(A, B):\n",
        "    # we use the simple rules introduced in class, and scan both shapes backwards\n",
        "    i_a = len(A.shape)-1 # index into A's shape\n",
        "    i_b = len(B.shape)-1 # index into B's shape\n",
        "    size =  [] # list to record the resulting shape\n",
        "    while i_a >= 0 or i_b >= 0:\n",
        "        \n",
        "        # check for cases in which one of the dimensions is missing\n",
        "        if i_a < 0:\n",
        "            size.insert(0, B.shape[i_b])\n",
        "        elif i_b < 0:\n",
        "            size.insert(0, A.shape[i_a])\n",
        "\n",
        "        # check for the case in which dimensions are equal\n",
        "        elif A.shape[i_a] == B.shape[i_b]:\n",
        "            size.insert(0, A.shape[i_a])\n",
        "\n",
        "        # check for cases in which one of the dimensions is a singletom\n",
        "        elif A.shape[i_a] == 1:\n",
        "            size.insert(0, B.shape[i_b])\n",
        "        elif B.shape[i_b] == 1:\n",
        "            size.insert(0, A.shape[i_a])\n",
        "\n",
        "        # otherwise - dimensions are not compatible\n",
        "        else:\n",
        "            return False, None\n",
        "\n",
        "        # update indices for the next iteration\n",
        "        i_a -= 1\n",
        "        i_b -= 1\n",
        "    \n",
        "    return True, size"
      ],
      "metadata": {
        "id": "Hlpy8CHB5rwH"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part C - Broadcasting Tensors Together"
      ],
      "metadata": {
        "id": "cCxGSL4iFEfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "my_broadcast_tensors : broadcasts 2 tensors together\n",
        "Input:\n",
        "  > 'A' & 'B' - the tensor to broadcast.\n",
        "Returns: 'A_new', 'B_new' - the broadcasted versions of 'A' and 'B'.\n",
        "Throws an assertion error if tensors cannot be broadcasted together\n",
        "'''\n",
        "def my_broadcast_tensors(A, B):\n",
        "    # calculate the broadcasted size\n",
        "    is_possible, size = are_broadcastible(A, B)\n",
        "\n",
        "    # make sure they're broadcastible\n",
        "    assert is_possible, ('tensors of shapes ' + str(tuple(A.shape)) + ' and ' +\n",
        "                         str(tuple(B.shape)) + ' cannot be broadcasted together')\n",
        "    \n",
        "    # and broadcast them\n",
        "    return expand_to(A, size), expand_to(B, size)"
      ],
      "metadata": {
        "id": "PSpXrDiSFLJy"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part D - Comparing Our Interface to the Official Interface"
      ],
      "metadata": {
        "id": "1dgkCpU0Lkq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we compare our implementation of the broadcasting interface to the official PyTorch implementation."
      ],
      "metadata": {
        "id": "qLEnZHa5MvaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "test_funcs : runs two implementations of the same functionality.\n",
        "Input:\n",
        "  > 'imp1' - a function representing the first implementation.\n",
        "  > 'imp2' - a function representing the second implementation\n",
        "  > 'A' & 'B' - input to both functions.\n",
        "Note: we assume both functions accept only 2 arguments - 'A' and 'B'.\n",
        "Returns (result1, exc1, result2, exc2) where for each i=1,2:\n",
        "  > 'resulti' is the output of the first implementation.\n",
        "  > 'exci' is a boolean stating whether an exception occured while running 'impi'.\n",
        "'''\n",
        "def test_funcs(imp1, imp2, A, B):\n",
        "    # try 1st implementation; set exc_1 to True if an exception occured\n",
        "    exc1 = False\n",
        "    result1 = None\n",
        "    try: result1 = imp1(A, B)\n",
        "    except: exc1 = True\n",
        "\n",
        "    # try the 2nd implementation; set exc2 to True if an exception occured\n",
        "    exc2 = False\n",
        "    result2 = None\n",
        "    try: result2 = imp2(A, B)\n",
        "    except: exc2 = True\n",
        "\n",
        "    return result1, exc1, result2, exc2"
      ],
      "metadata": {
        "id": "IM40019tOt03"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In each of the following cells we test our implementations against the official implementations on different test examples using 'test_funcs' defined above."
      ],
      "metadata": {
        "id": "AFoZ58UmOybC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if our 'expand_as' works the same as the official 'expand_as' on the\n",
        "# following A,B input:\n",
        "A = torch.arange(    3*1*5).reshape((    3,1,5))\n",
        "B = torch.arange(1*2*3*4*5).reshape((1,2,3,4,5))\n",
        "\n",
        "# run test:\n",
        "result1, exc1, result2, exc2 = test_funcs(my_expand_as, torch.Tensor.expand_as,  A, B)\n",
        "if torch.equal(result1, result2):\n",
        "    print('Test passed successfuly; output shape:', tuple(result1.shape), '.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNhIdJUqGq5x",
        "outputId": "502e7aef-4a00-4beb-9bc2-599e84b3d5cb"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed successfuly; output shape: (1, 2, 3, 4, 5) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if our 'expand_as' works the same as the official 'expand_as' on the\n",
        "# following A,B input:\n",
        "A = torch.arange(    2*1*5).reshape((    2,1,5))\n",
        "B = torch.arange(1*2*3*4*5).reshape((1,2,3,4,5))\n",
        "\n",
        "# run test:\n",
        "result1, exc1, result2, exc2 = test_funcs(my_expand_as, torch.Tensor.expand_as,  A, B)\n",
        "if exc1 == True and exc2 == True:\n",
        "    print('Test passed successfuly; both implementations threw an exception.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC-Vm0dHbFyl",
        "outputId": "9060584f-021b-4bfd-bf0f-e0ac6000e13f"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed successfuly; both implementations threw an exception.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if our 'broadcast_tensors' works the same as the official 'broadcast_tensors' \n",
        "# on the following A,B input:\n",
        "A = torch.arange(    3*1*5).reshape((    3,1,5))\n",
        "B = torch.arange(1*2*3*4*5).reshape((1,2,3,4,5))\n",
        "\n",
        "# run test:\n",
        "result1, exc1, result2, exc2 = test_funcs(my_broadcast_tensors,\n",
        "                                          torch.broadcast_tensors,\n",
        "                                          A, B)\n",
        "result1_A, result1_B = result1\n",
        "result2_A, result2_B = result2\n",
        "if torch.equal(result1_A, result2_A) and torch.equal(result1_B, result2_B):\n",
        "    print('Test passed successfully; output shape:', tuple(result1_A.shape), '.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAvRCfp8bpFn",
        "outputId": "69cadbc8-c1bf-497e-b869-3ded6d5af3a9"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed successfully; output shape: (1, 2, 3, 4, 5) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if our 'broadcast_tensors' works the same as the official 'broadcast_tensors' \n",
        "# on the following A,B input:\n",
        "A = torch.arange(    3*1*5).reshape((    3,1,5))\n",
        "B = torch.arange(1*2*1*4*5).reshape((1,2,1,4,5))\n",
        "\n",
        "# run test:\n",
        "result1, exc1, result2, exc2 = test_funcs(my_broadcast_tensors,\n",
        "                                          torch.broadcast_tensors,\n",
        "                                          A, B)\n",
        "result1_A, result1_B = result1\n",
        "result2_A, result2_B = result2\n",
        "if torch.equal(result1_A, result2_A) and torch.equal(result1_B, result2_B):\n",
        "    print('Test passed successfully; output shape:', tuple(result1_A.shape), '.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwO4o0bxdDK9",
        "outputId": "f205fd95-f410-4234-c117-3ce9f53b7e47"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed successfully; output shape: (1, 2, 3, 4, 5) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if our 'broadcast_tensors' works the same as the official 'broadcast_tensors' \n",
        "# on the following A,B input:\n",
        "A = torch.arange(    3*2*5).reshape((    3,2,5))\n",
        "B = torch.arange(1*2*1*4*5).reshape((1,2,1,4,5))\n",
        "\n",
        "# run test:\n",
        "result1, exc1, result2, exc2 = test_funcs(my_broadcast_tensors,\n",
        "                                          torch.broadcast_tensors,\n",
        "                                          A, B)\n",
        "if exc1 == True and exc2 == True:\n",
        "    print('Test passed successfuly; both implementations threw an exception.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv5fh-vlHdR1",
        "outputId": "f61f8335-0074-4f00-f8ee-6cbd0b5b0301"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed successfuly; both implementations threw an exception.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3\n",
        "In this question I implement a simple  version of the autograd system of PyTorch.\n",
        "I decided to support **both unary and binary operations** on scalars (with a simple mechanism to actually support any n-ary operation)."
      ],
      "metadata": {
        "id": "3j4HaPwie8HO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The system is based on the 'MyScalar' class that represents scalars.\n",
        "\n",
        "Each scalar object 's' has 4 attributes:\n",
        "* s.value: the floating-point value of the scalar.\n",
        "* s.parents: A list of pairs (parent, parent_grad) where 'parent' is a pointer to another 'MyScalar' objects from which 's' was directly created (using some mathematical operation), and 'parent_d' is the **immediate** derivative of 's' with respect to 'parent'. May be empty for a \"root\" scalar.\n",
        "* s.grads_cache: dictionary of derivatives of 's' with respect to every scalar it depends on - indexed by IDs of the corresponding 'MyScalar' objects. May be None if no function had to scan backwards the scalar yet, or if the user does not wish to cache the scalar's derivatives. More detail later.\n",
        "* s.to_cache_grads: a boolean stating whether the user wishes to cache derivatives of 's' with respect to every ancestor of it (it does not mean that the derivatives have already been cached).\n",
        "\n",
        "The auto-grad system offered by the 'get_gradient' routine returns a dictionary of derivatives of 's' with respect to scalars it depends on; indexed by IDs of the corresponding 'MyScalar' objects.\n",
        "\n",
        "The system uses recursive applications of the chain rule to generate those derivatives, using the following algorithm:\n",
        "\n",
        "\n",
        "```\n",
        "1. Recursively generate the derivatives with respect to each of the parents of\n",
        "   's' (found in s.parents).\n",
        "2. for each scalar 't' on which some parent 'p' of 's' is dependant, calculate\n",
        "   the derivative of 's' with respect to 't' through 'p' as: [dp/dt] * [dt/ds],\n",
        "   and add it to the derivative [ds/dt] calculated so far.\n",
        "3. At last - return a dictionary of the total derivatives, with an entry for\n",
        "   the derivative of 's' with respect to itself (1.0).\n",
        "\n",
        "```\n",
        "Note that we **add** derivatives, due to the fact that the same scalar 't' may affect 's' throguh different parents 'p' of 's'.\n",
        "\n",
        "Because traversing through the dependences tree may be very costly, the user may wish to cache derivatives of a frequently used scalar with respect to each one of its ancestors (instead of calculating them again and again each time one of its descendants wants its gradients). For that reason I decided to add this caching option, as described in the next cell in th API of the class."
      ],
      "metadata": {
        "id": "LaV_lDmMooIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "MyScalar : represents a scalar in my auto-grad system.\n",
        "I highly recommend to manipulate MyScalars objects only through the functional\n",
        "  interface this class offers. For the sake of simplicity, scalars are immutable\n",
        "  in this interface (and so is their derivatives cache).\n",
        "'''\n",
        "class MyScalar:\n",
        "\n",
        "    '''\n",
        "    Constructor : creates a new scalar object with the specified attributes.\n",
        "    Attributes:\n",
        "      > 'value' - value of the scalar.\n",
        "      > 'parents' - list of (parent, parent_grad) pairs where 'parent' is a pointer\n",
        "        to another MyScalar object from which this scalar was directly created\n",
        "        (via some mathematical operation), and 'parent_grad' is the derivative of\n",
        "        this scalar with respect to that parent. Default: empty list (no dependencies).\n",
        "      > 'to_cache_grads' - a boolean stating whether the user wishes to cache the\n",
        "        derivatives of the scalar with respect to every ancestor of it, when they\n",
        "        will be calculated. I recommend to set it to True if it has a lot of \n",
        "        descendants whose gradients you wish to calculate later. Default: False.\n",
        "    '''\n",
        "    def __init__(self, value, parents=[], to_cache_grads=False):\n",
        "        # initialization:\n",
        "        self.value = value\n",
        "        self.grads_cache = None # nothing is cached yet\n",
        "        self.to_cache_grads = to_cache_grads\n",
        "\n",
        "        # create a \"semi-deep\" copy of 'parents' (necessary if 'parents' is a\n",
        "        # list of lists of 2 variables and not a list of tuples, in which case\n",
        "        # a shallow copy is not enough and a deep copy is too much):\n",
        "        self.parents = []\n",
        "        for parent, parent_grad in parents:\n",
        "            self.parents.append((parent, parent_grad))\n",
        "    \n",
        "    \n",
        "    '''\n",
        "    set_to_cache_grads : sets the 'to_cache_grads' attribute of the MyScalar object.\n",
        "    Input:\n",
        "      > 'to_cache_grads' - the value to assign: a boolean stating whether the user\n",
        "         wishes to cache the derivatives of the scalar with respect to every ancestor \n",
        "        of it, when they will be calculated. I recommend to set it to True if it has \n",
        "        a lot of  descendants whose gradients you wish to calculate later.\n",
        "    Note: if derivatives were already cached and now caching is disabled, the cache is\n",
        "      discarded.\n",
        "    '''\n",
        "    def set_to_cache_grads(self, to_cache_grads):\n",
        "        # set attribute\n",
        "        self.to_cache_grads = to_cache_grads\n",
        "        # discard cache in case caching is now disabled\n",
        "        if to_cache_grads == False:\n",
        "            self.grads_cache = None\n",
        "\n",
        "\n",
        "    '''\n",
        "    get_value : returns the value of the scalar.\n",
        "    '''\n",
        "    def get_value(self):\n",
        "        return self.value\n",
        "\n",
        "\n",
        "    '''\n",
        "    get_gradient : calculates gradients of this scalar with respect to every\n",
        "      other scalar it depend on.\n",
        "    Returns a dictionary of derivatives, indexed by the IDs of the corresponding\n",
        "      MyScalar objects.\n",
        "    '''\n",
        "    def get_gradient(self):\n",
        "        # recalculate derivatives if they are not cached:\n",
        "        if self.grads_cache == None:\n",
        "            grads_of_self = {} # to hold the result\n",
        "\n",
        "            # get ancestors' derivatives through direct parents\n",
        "            for (parent, parent_grad) in self.parents:\n",
        "                grads_of_parent = parent.get_gradient()\n",
        "\n",
        "                # record ancestors' derivatives through 'parent'\n",
        "                for ancestor_id in grads_of_parent.keys():\n",
        "                    # make sure there is a record of the ancestor, and update it:\n",
        "                    if ancestor_id not in grads_of_self.keys():\n",
        "                        grads_of_self[ancestor_id] = 0.0\n",
        "                    grads_of_self[ancestor_id] += parent_grad * grads_of_parent[ancestor_id]\n",
        "                    # = d(self)/d(parent) * d(parent) / d(ancestor)\n",
        "            \n",
        "            # add entry for 'self' itself:\n",
        "            grads_of_self[id(self)] = 1.0\n",
        "\n",
        "            # cache a copy of the derivatives if needed\n",
        "            if self.to_cache_grads == True:\n",
        "                self.grads_cache = grads_of_self.copy()\n",
        "            \n",
        "            return grads_of_self\n",
        "\n",
        "        # in case derivatives are cached - return copy of the cache\n",
        "        else:\n",
        "            return self.grads_cache.copy()\n",
        "    \n",
        "\n",
        "    '''\n",
        "    __str__ : returns a string representation of the scalar, rounded to 'd' digits after\n",
        "      the decimal point (default: d=2)\n",
        "    '''\n",
        "    def __str__(self, d=2):\n",
        "        return str(round(self.value, d))\n",
        "\n",
        "\n",
        "    '''\n",
        "    __float__ : returns the float value of the scalar.\n",
        "    '''\n",
        "    def __float__(self):\n",
        "        return float(self.value)\n",
        "\n",
        "\n",
        "    '''\n",
        "    __add__ : adds another scalar to this scalar, and returns a new MyScalar with the result.\n",
        "    Input:\n",
        "      > 'other' - the other scalar to add. May be a simple numeric primitive or another\n",
        "        MyScalar - in which case the new MyScalar will be a child of 'other' as well as of\n",
        "        'self' in the gradient dependencies tree.\n",
        "      > 'to_cache_grads' - a boolean stating whether the user wishes to cache gradients\n",
        "        of the resulting MyScalar (more detail in MyScalar.__init__ documentation).\n",
        "        Default: False.\n",
        "    '''\n",
        "    def __add__(self, other, to_cache_grads=False):\n",
        "        # store parents and calculate gradients\n",
        "        parents = [(self, 1.0)] # d(self+other)/d(self)=1\n",
        "        if type(other) == MyScalar:\n",
        "          parents.append((other, 1.0)) # d(self+other)/d(other)=1\n",
        "\n",
        "        # create and return new MyScalar\n",
        "        return MyScalar(float(self) + float(other),\n",
        "                        parents, to_cache_grads)\n",
        "\n",
        "    '''\n",
        "    __radd__ : same as '__add__'.\n",
        "    '''\n",
        "    def __radd__(self, other, to_cache_grads=False):\n",
        "        return self.__add__(other, to_cache_grads)\n",
        "\n",
        "\n",
        "    '''\n",
        "    __mul__ : multiplies this scalar with another scalar, and returns a new MyScalar with the\n",
        "      result.\n",
        "    Input:\n",
        "      > 'other' - the other scalar to multiply. May be a simple numeric primitive or another\n",
        "        MyScalar - in which case the new MyScalar will be a child of 'other' (as well as of\n",
        "        'self') in the gradient dependencies tree.\n",
        "      > 'to_cache_grads' - a boolean stating whether the user wishes to cache gradients\n",
        "        of the resulting MyScalar (more detail in MyScalar.__init__ documentation).\n",
        "        Default: False.\n",
        "    '''\n",
        "    def __mul__(self, other, to_cache_grads=False):\n",
        "        # store parents and calculate gradients\n",
        "        parents = [(self, float(other))] # d(self*other)/d(self)=other\n",
        "        if type(other) == MyScalar:\n",
        "          parents.append((other, float(self))) # d(self*other)/d(other)=self\n",
        "\n",
        "        # create and return new MyScalar\n",
        "        return MyScalar(float(self) * float(other),\n",
        "                        parents, to_cache_grads)\n",
        "    \n",
        "    '''\n",
        "    __rmul__ : same as '__mul__'.\n",
        "    '''\n",
        "    def __rmul__(self, other, to_cache_grads=False):\n",
        "        return self.__mul__(other, to_cache_grads)\n",
        "\n",
        "\n",
        "    '''\n",
        "    __truediv__ : divides this scalar by another scalar, and returns a new MyScalar with \n",
        "      the result.\n",
        "    Input:\n",
        "      > 'other' - the scalar to divide by. May be a simple numeric primitive or another\n",
        "        MyScalar - in which case the new MyScalar will be a child of 'other' (as well\n",
        "        as of 'self') in the gradient dependencies tree.\n",
        "      > 'to_cache_grads' - a boolean stating whether the user wishes to cache gradients\n",
        "        of the resulting MyScalar (more detail in MyScalar.__init__ documentation).\n",
        "        Default: False.\n",
        "    '''\n",
        "    def __truediv__(self, other, to_cache_grads=False):\n",
        "        # store parents and calculate gradients\n",
        "        parents = [(self, 1/float(other))] # d(self/other)/d(self)=1/other\n",
        "        if type(other) == MyScalar:\n",
        "            parents.append((other,\n",
        "                          -float(self)/float(other)**2)) # d(self/other)/d(other)=-self/(other)**2\n",
        "\n",
        "        # create and return new MyScalar\n",
        "        return MyScalar(float(self) / float(other),\n",
        "                        parents, to_cache_grads)\n",
        "    \n",
        "    '''\n",
        "    __rtruediv__ : divides another scalar by this scalar, and returns a new MyScalar with\n",
        "      the result.\n",
        "    Input:\n",
        "      > 'other' - the scalar to divide. May be a simple numeric primitive or another\n",
        "        MyScalar - in which case the new MyScalar will be a child of 'other' (as well\n",
        "        as of 'self') in the gradient dependencies tree.\n",
        "      > 'to_cache_grads' - a boolean stating whether the user wishes to cache gradients\n",
        "        of the resulting MyScalar (more detail in MyScalar.__init__ documentation).\n",
        "        Default: False.\n",
        "    '''\n",
        "    def __rtruediv__(self, other, to_cache_grads=False):\n",
        "        # store parents and calculate gradients\n",
        "        parents = [(self, -float(other)/float(self)**2)] # d(other/self)/d(self)=-other/self**2\n",
        "        if type(other) == MyScalar:\n",
        "            parents.append((other, 1/float(self))) # d(other/self)/d(other)=1/self\n",
        "\n",
        "        # create and return new MyScalar\n",
        "        return MyScalar(float(other) / float(self),\n",
        "                        parents, to_cache_grads)\n",
        "\n",
        "    \n",
        "    '''\n",
        "    __pow__ : raises this scalar to the power of another scalar, and returns a new\n",
        "        MyScalar with the result.\n",
        "    Input:\n",
        "      > 'other' - the other scalar (the power). May be a simple numeric primitive or \n",
        "        another MyScalar - in which case the new MyScalar will be a child of 'other'\n",
        "        (as well as of 'self') in the gradient dependencies tree.\n",
        "      > 'to_cache_grads' - a boolean stating whether the user wishes to cache gradients\n",
        "        of the resulting MyScalar (more detail in MyScalar.__init__ documentation).\n",
        "        Default: False.\n",
        "    '''\n",
        "    def __pow__(self, other, to_cache_grads=False):\n",
        "        # store parents and calculate gradients\n",
        "        parents = [(self, float(other) * (float(self)**(float(other)-1)))]\n",
        "                          # d(self**other)/d(self)=other*(self)**(other-1)\n",
        "        if type(other) == MyScalar:\n",
        "            parents.append((other, float(self)**float(other) * math.log(float(self))))\n",
        "                                  # d(self**other)/d(other)=self**other*ln(self)\n",
        "\n",
        "        # create and return new MyScalar\n",
        "        return MyScalar(float(self)**float(other),\n",
        "                        parents, to_cache_grads)\n",
        "    \n",
        "    '''\n",
        "    __rpow__ : raises another scalar to the power of this scalar, and returns a new\n",
        "        MyScalar with the result.\n",
        "    Input:\n",
        "      > 'other' - the other scalar (the base). May be a simple numeric primitive or \n",
        "        another MyScalar - in which case the new MyScalar will be a child of 'other'\n",
        "        (as well as of 'self') in the gradient dependencies tree.\n",
        "      > 'to_cache_grads' - a boolean stating whether the user wishes to cache gradients\n",
        "        of the resulting MyScalar (more detail in MyScalar.__init__ documentation).\n",
        "        Default: False.\n",
        "    '''\n",
        "    def __rpow__(self, other, to_cache_grads=False):\n",
        "        # store parents and calculate gradients\n",
        "        parents = [(self, float(other)**float(self) * math.log(float(other)))]\n",
        "                          # d(other**self)/d(self)=other**self*ln(other)\n",
        "        if type(other) == MyScalar:\n",
        "            parents.append((other, float(self) * (float(other)**(float(self)-1))))\n",
        "                                   # d(other**self)/d(other)=self*(other)**(self-1)\n",
        "                                 \n",
        "\n",
        "        # create and return new MyScalar\n",
        "        return MyScalar(float(other)**float(self),\n",
        "                        parents, to_cache_grads)\n",
        "        \n",
        "    \n",
        "    '''\n",
        "    exp : raises e to the power of the scalar, and returns a new MyScalar object with\n",
        "      the result.\n",
        "    Input:\n",
        "      > 'to_cache_grads' - a boolean stating whether the user wishes to cache gradients\n",
        "        of the resulting MyScalar (more detail in MyScalar.__init__ documentation).\n",
        "        Default: False.\n",
        "    '''\n",
        "    def exp(self, to_cache_grads=False):\n",
        "        # simply use __rpow__ defined above\n",
        "        return self.__rpow__(math.e, to_cache_grads)\n",
        "    \n",
        "    '''\n",
        "    log : calculates the natural log of the scalar and returns a new MyScalar object with\n",
        "      the result.\n",
        "    Input:\n",
        "      > 'to_cache_grads' - a boolean stating whether the user wishes to cache gradients\n",
        "        of the resulting MyScalar (more detail in MyScalar.__init__ documentation).\n",
        "        Default: False.\n",
        "    '''\n",
        "    def log(self, to_cache_grads=False):\n",
        "        return MyScalar(math.log(self.value), # result\n",
        "                        [(self, 1/self.value)], # = d(log(self))/self = 1/self\n",
        "                        to_cache_grads)\n",
        "    \n",
        "    '''\n",
        "    sin : calculates the sin of the scalar and returns a new MyScalar object with the result.\n",
        "    Input:\n",
        "      > 'to_cache_grads' - a boolean stating whether the user wishes to cache gradients\n",
        "        of the resulting MyScalar (more detail in MyScalar.__init__ documentation).\n",
        "        Default: False.\n",
        "    '''\n",
        "    def sin(self, to_cache_grads=False):\n",
        "        return MyScalar(math.sin(self.value), # result\n",
        "                        [(self, math.cos(self.value))], # = d(sin(self))/self = cos(self)\n",
        "                        to_cache_grads)\n",
        "    \n",
        "    '''\n",
        "    cos : calculates the cos of the scalar and returns a new MyScalar object with the result.\n",
        "    Input:\n",
        "      > 'to_cache_grads' - a boolean stating whether the user wishes to cache gradients\n",
        "        of the resulting MyScalar (more detail in MyScalar.__init__ documentation).\n",
        "        Default: False.\n",
        "    '''\n",
        "    def cos(self, to_cache_grads=False):\n",
        "        return MyScalar(math.cos(self.value), # result\n",
        "                        [(self, -math.sin(self.value))], # = d(cos(self))/self = -sin(self)\n",
        "                        to_cache_grads)"
      ],
      "metadata": {
        "id": "y95ZwsIZ1Qw0"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the 'MyScalar' class, we can use it to calculate gradients automatically. In the following cell I demonstarte the different functions that the class supplies, and some derivatives of those functions."
      ],
      "metadata": {
        "id": "fzq3QTs1qxMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we define a few \"root\" objects of MyScalar\n",
        "one = MyScalar(1)\n",
        "two = MyScalar(2)\n",
        "ln_5 = MyScalar(math.log(5))\n",
        "pi_over_2 = MyScalar(math.pi / 2)\n",
        "\n",
        "# we use them to calculate new scalars using various operations\n",
        "one_plus_two = one + two\n",
        "one_times_two = one * two\n",
        "one_over_two = one / two\n",
        "two_to_the_one = two ** one\n",
        "exp_ln_5 = ln_5.exp()\n",
        "ln_of_two = two.log()\n",
        "cos_pi_over_2 = pi_over_2.cos()\n",
        "sin_pi_over_2 = pi_over_2.sin()\n",
        "\n",
        "# and calculate some derivatives\n",
        "print('d(one+two)/d(one) =', one_plus_two.get_gradient()[id(one)])\n",
        "print('d(one*two)/d(two) =', one_times_two.get_gradient()[id(two)])\n",
        "print('d(one/two)/d(two) =', one_over_two.get_gradient()[id(two)])\n",
        "print('d(two**one)/d(one) =', two_to_the_one.get_gradient()[id(one)])\n",
        "print('d(exp(ln_5))/d(ln_5) =', exp_ln_5.get_gradient()[id(ln_5)])\n",
        "print('d(ln(two))/d(two) =', ln_of_two.get_gradient()[id(two)])\n",
        "print('d(cos(pi_over_2))/d(pi_over_2)', cos_pi_over_2.get_gradient()[id(pi_over_2)])\n",
        "print('d(sin(pi_over_2))/d(pi_over_2)', sin_pi_over_2.get_gradient()[id(pi_over_2)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZj69-yJrFnM",
        "outputId": "81783da2-484b-4bd3-e55b-20eed9e227bb"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d(one+two)/d(one) = 1.0\n",
            "d(one*two)/d(two) = 1.0\n",
            "d(one/two)/d(two) = -0.25\n",
            "d(two**one)/d(one) = 1.3862943611198906\n",
            "d(exp(ln_5))/d(ln_5) = 4.999999999999999\n",
            "d(ln(two))/d(two) = 0.5\n",
            "d(cos(pi_over_2))/d(pi_over_2) -1.0\n",
            "d(sin(pi_over_2))/d(pi_over_2) 6.123233995736766e-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try to differentiate a more complicated function - the logistic sigmoid:"
      ],
      "metadata": {
        "id": "aWE14yBcxLvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculations:\n",
        "x = MyScalar(0)\n",
        "sigmoid_of_x = 1 / (1 + math.e ** ((-1) * x))\n",
        "derivative = sigmoid_of_x.get_gradient()[id(x)]\n",
        "# message:\n",
        "print('x =', x)\n",
        "print('sigmoid at x =', sigmoid_of_x)\n",
        "print('derivative =', derivative)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBC7Eq1oxqQs",
        "outputId": "98089463-a98a-4567-a70f-703363200142"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = 0\n",
            "sigmoid at x = 0.5\n",
            "derivative = 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try antoher complicated function, in which the same scalar affects the result in multiple places:"
      ],
      "metadata": {
        "id": "MH32r4zl0-xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculations:\n",
        "x = MyScalar(2)\n",
        "f_of_x = x + (2 * x + x) / (x * x)\n",
        "derivative = f_of_x.get_gradient()[id(x)]\n",
        "# message:\n",
        "print('x =', x)\n",
        "print('x + (2x+x)/(x*x) =', f_of_x)\n",
        "print('derivative =', derivative)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWUpMAw21OVP",
        "outputId": "f10f2aec-5f9d-4ffb-c9c2-c637a105ae7a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = 2\n",
            "x + (2x+x)/(x*x) = 3.5\n",
            "derivative = 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, in the follwong cells we will compare our system to the official PyTorch autograd system.\n",
        "\n",
        "In the next cell we define 'test_autograd' to test the system with arbitrary mathematical functions (as a composition of the elementry functions), and in the one that follows we test different functions at different 'x' values."
      ],
      "metadata": {
        "id": "V8fvnikC16En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "test_autograd : test the autograd system implemented in 'MyScalar' class, against\n",
        "  the official PyTorch system.\n",
        "Input:\n",
        "  > 'f' - any function (of one variable) composed of mathematical operations implemented\n",
        "    by both the torch.Tensor class and the MyScalar class.\n",
        "    For instance: f = lambda x: x.log() * x.cos() + x / (x+x)**2\n",
        "  > 'x' - the (primitive numeric) value at which we wish to test the value of f.\n",
        "  > 'title' - a string added as the title of the test (or None for no title).\n",
        "Prints a message regarding the success of the test. Rounds results to 3 digits after the\n",
        "  decimal point.\n",
        "'''\n",
        "def test_autograd(f, x, title=None):\n",
        "    x = float(x) # so torch autograd can activate\n",
        "\n",
        "    # print title\n",
        "    if title != None:\n",
        "        print(title)\n",
        "\n",
        "    # perform the necessary calculations with the PyTorch tensor iterface:\n",
        "    tensor_x = torch.tensor(x, requires_grad=True)\n",
        "    tensor_fx = f(tensor_x)\n",
        "    tensor_fx.backward()\n",
        "    tensor_df = tensor_x.grad\n",
        "    tensor_fx_val = round(tensor_fx.item(), 3)\n",
        "    tensor_df_val = round(tensor_df.item(), 3)\n",
        "\n",
        "    # perform the necessary calculation with the MyScalar interface:\n",
        "    myscalar_x = MyScalar(x)\n",
        "    myscalar_fx = f(myscalar_x)\n",
        "    myscalar_df = myscalar_fx.get_gradient()[id(myscalar_x)]\n",
        "    myscalar_fx_val = round(myscalar_fx.get_value(), 3)\n",
        "    myscalar_df_val = round(myscalar_df, 3)\n",
        "\n",
        "    # and finally - compare the results\n",
        "    if tensor_fx_val == myscalar_fx_val:\n",
        "        print('Function evaluated correctly to', myscalar_fx_val)\n",
        "    else:\n",
        "        print('Function evaluation failed: expected', tensor_fx_val, 'but got',\n",
        "               myscalar_fx_val)\n",
        "    if tensor_df_val == myscalar_df_val:\n",
        "        print('Derivative evaluated correctly to', myscalar_df_val)\n",
        "    else:\n",
        "        print('Derivative evaluation failed: expected', tensor_df_val, 'but got',\n",
        "              myscalar_df_val)\n",
        "    print('')"
      ],
      "metadata": {
        "id": "YwkXUOJ-2_cO"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_autograd(lambda x: x*x, x = 5,\n",
        "              title='testing funciton x*x at x=5')\n",
        "\n",
        "test_autograd(lambda x: (x.cos()+x.log())**2, x=3,\n",
        "              title='testing funciton (cos(x)+log(x))**2 at x=3')\n",
        "\n",
        "test_autograd(lambda x: x ** (x / x.log()), x=0.5,\n",
        "              title='testing function x ** (x/log(x)) at x=0.5')\n",
        "\n",
        "test_autograd(lambda x: 1 / (1 + ((-1) * x).exp()), x=-2,\n",
        "              title='testing logistic sigmoid function at x=-2')\n",
        "\n",
        "test_autograd(lambda x: x.cos()+1+x.sin()**2, x=-5,\n",
        "              title='testing function cos(x)+1+sin(x)**2 at x=-5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhPc4Ys36X_8",
        "outputId": "d96e8bf4-c57b-4a24-ebb9-e7ffab691ddc"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing funciton x*x at x=5\n",
            "Function evaluated correctly to 25.0\n",
            "Derivative evaluated correctly to 10.0\n",
            "\n",
            "testing funciton (cos(x)+log(x))**2 at x=3\n",
            "Function evaluated correctly to 0.012\n",
            "Derivative evaluated correctly to 0.042\n",
            "\n",
            "testing function x ** (x/log(x)) at x=0.5\n",
            "Function evaluated correctly to 1.649\n",
            "Derivative evaluated correctly to 1.649\n",
            "\n",
            "testing logistic sigmoid function at x=-2\n",
            "Function evaluated correctly to 0.119\n",
            "Derivative evaluated correctly to 0.105\n",
            "\n",
            "testing function cos(x)+1+sin(x)**2 at x=-5\n",
            "Function evaluated correctly to 2.203\n",
            "Derivative evaluated correctly to -0.415\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good luck ^_^\n",
        "\n",
        "Nadav.\n",
        "\n"
      ],
      "metadata": {
        "id": "jvqaaq8K_Rzh"
      }
    }
  ]
}